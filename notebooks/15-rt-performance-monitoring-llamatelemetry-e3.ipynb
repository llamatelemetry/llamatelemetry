{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook 15: Real-Time Inference Monitoring & Performance Analysis\n\n**Live Performance Dashboards with llama.cpp Metrics + Plotly**\n\n---\n\n## Objectives Demonstrated\n\nâœ… **CUDA Inference** (GPU 0) - Continuous inference workload\n\nâœ… **LLM Observability** (GPU 0) - llama.cpp /metrics endpoint + CUDA monitoring\n\nâœ… **Visualizations** (GPU 1) - Real-time Plotly dashboards with live updates\n\n---\n\n## Overview\n\nThis notebook demonstrates **real-time performance monitoring** of LLM inference by continuously polling llama.cpp's built-in `/metrics` endpoint and NVIDIA's GPU metrics, then visualizing them as live-updating Plotly dashboards on GPU 1.\n\n**What You'll Learn:**\n- Access llama.cpp's Prometheus `/metrics` endpoint\n- Monitor GPU utilization with `nvidia-smi` and `pynvml`\n- Poll llama.cpp `/slots` endpoint for request queue monitoring\n- Create live-updating Plotly dashboards with `plotly.graph_objects.FigureWidget`\n- Identify performance bottlenecks and optimization opportunities\n- Benchmark different configurations (batch size, context length, etc.)\n\n**Time:** 30 minutes\n\n**Difficulty:** Intermediate-Advanced\n\n**VRAM:** GPU 0: 5-8 GB, GPU 1: 1-2 GB","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np\nimport pandas as pd\nimport os\n\n# Input data files are available in the read-only \"../input/\" directory\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T06:52:33.359326Z","iopub.execute_input":"2026-02-09T06:52:33.360050Z","iopub.status.idle":"2026-02-09T06:52:33.649373Z","shell.execute_reply.started":"2026-02-09T06:52:33.360012Z","shell.execute_reply":"2026-02-09T06:52:33.648602Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T06:52:33.668780Z","iopub.execute_input":"2026-02-09T06:52:33.669095Z","iopub.status.idle":"2026-02-09T06:52:33.672892Z","shell.execute_reply.started":"2026-02-09T06:52:33.669072Z","shell.execute_reply":"2026-02-09T06:52:33.672111Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Part 1: Setup & Dependencies","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 1: Verify Dual GPU Environment\n# ==============================================================================\n\nimport subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"ğŸ” SPLIT-GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\ngpus = result.stdout.strip().split('\\n')\nprint(f\"\\nğŸ“Š Detected {len(gpus)} GPU(s):\")\nfor gpu in gpus:\n    print(f\"   {gpu}\")\n\nif len(gpus) >= 2:\n    print(\"\\nâœ… Dual T4 ready for split-GPU operation!\")\n    print(\"   GPU 0 â†’ llama-server (GGUF model inference)\")\n    print(\"   GPU 1 â†’ RAPIDS/Graphistry (architecture visualization)\")\nelse:\n    print(\"\\nâš ï¸ Need 2 GPUs for split operation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T06:58:27.815835Z","iopub.execute_input":"2026-02-09T06:58:27.816216Z","iopub.status.idle":"2026-02-09T06:58:27.864123Z","shell.execute_reply.started":"2026-02-09T06:58:27.816182Z","shell.execute_reply":"2026-02-09T06:58:27.863186Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ” SPLIT-GPU ENVIRONMENT CHECK\n======================================================================\n\nğŸ“Š Detected 2 GPU(s):\n   0, Tesla T4, 15360 MiB, 14913 MiB\n   1, Tesla T4, 15360 MiB, 14913 MiB\n\nâœ… Dual T4 ready for split-GPU operation!\n   GPU 0 â†’ llama-server (GGUF model inference)\n   GPU 1 â†’ RAPIDS/Graphistry (architecture visualization)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==============================================================================\n# Step 2: Install llamatelemetry v0.1.0\n# ==============================================================================\nprint(\"ğŸ“¦ Installing dependencies...\")\n\n# Install llamatelemetry v0.1.0\n!pip install -q https://github.com/llamatelemetry/llamatelemetry/releases/download/v0.1.0/llamatelemetry-v0.1.0-source.tar.gz\n#!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\n# Install cuGraph for GPU-accelerated graph algorithms\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\" \"cudf-cu12==25.6.*\"\n\n# Install Graphistry for visualization\n!pip install -q \"graphistry[ai]\"\n\n# Install additional utilities\n!pip install -q pyarrow pandas numpy scipy huggingface_hub\n\n# Verify installations\nimport llamatelemetry\nprint(f\"\\nâœ… llamatelemetry {llamatelemetry.__version__} installed\")\n\ntry:\n    import cudf, cugraph\n    print(f\"âœ… cuDF {cudf.__version__}\")\n    print(f\"âœ… cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"âœ… Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Graphistry: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T06:58:30.250691Z","iopub.execute_input":"2026-02-09T06:58:30.251181Z","iopub.status.idle":"2026-02-09T07:00:18.488650Z","shell.execute_reply.started":"2026-02-09T06:58:30.251153Z","shell.execute_reply":"2026-02-09T07:00:18.487839Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Installing dependencies...\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m763.5/763.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nğŸ¯ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2Ã— T4 Multi-GPU\n======================================================================\n\nğŸ® GPU Detected: Tesla T4 (Compute 7.5)\n  âœ… Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nğŸŒ Platform: Colab\n\nğŸ“¦ Downloading Kaggle 2Ã— T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\nâ¡ï¸  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nğŸ“¥ Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(â€¦):   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78bef6efb3fe40ea91905931e508065c"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Verifying SHA256 checksum...\n   âœ… Checksum verified\nğŸ“¦ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\nâœ… Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\nâœ… Binaries installed successfully!\n\n\nâœ… llamatelemetry 0.1.0 installed\nâœ… cuDF 25.06.00\nâœ… cuGraph 25.06.00\nâœ… Graphistry 0.50.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# OpenTelemetry (versions aligned to notebook 14)\n!pip install -q   opentelemetry-api==1.37.0   opentelemetry-sdk==1.37.0   opentelemetry-proto==1.37.0   opentelemetry-exporter-otlp-proto-common==1.37.0   opentelemetry-exporter-otlp-proto-grpc==1.37.0   opentelemetry-exporter-otlp-proto-http==1.37.0   rich==13.9.4   --upgrade-strategy=only-if-needed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:08:52.714923Z","iopub.execute_input":"2026-02-09T07:08:52.715843Z","iopub.status.idle":"2026-02-09T07:08:55.883289Z","shell.execute_reply.started":"2026-02-09T07:08:52.715798Z","shell.execute_reply":"2026-02-09T07:08:55.882501Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!pip -q install \"google-cloud-bigquery-storage>=2.30.0,<3.0.0\" --upgrade-strategy=only-if-needed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:08:45.810024Z","iopub.execute_input":"2026-02-09T07:08:45.810742Z","iopub.status.idle":"2026-02-09T07:08:49.407902Z","shell.execute_reply.started":"2026-02-09T07:08:45.810712Z","shell.execute_reply":"2026-02-09T07:08:49.407019Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.7/303.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================== \n# Step 3: Setup Secrets (Grafana OTLP + HuggingFace + Graphistry)\n# ============================================================================== \nimport os\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nimport graphistry\n\nsecrets = UserSecretsClient()\n\n# Grafana OTLP\nGRAFANA_OTLP_ENDPOINT = secrets.get_secret(\"GRAFANA_OTLP_ENDPOINT\").rstrip(\"/\")\nGRAFANA_OTLP_BASIC_B64 = secrets.get_secret(\"GRAFANA_OTLP_BASIC_B64\")\nGRAFANA_OTLP_INSTANCE_ID = secrets.get_secret(\"GRAFANA_OTLP_INSTANCE_ID\")\nGRAFANA_OTLP_TOKEN = secrets.get_secret(\"GRAFANA_OTLP_TOKEN\")\n\n# HuggingFace\nHF_TOKEN = secrets.get_secret(\"HF_TOKEN_2\")\n\n# Graphistry\nGRAPHISTRY_PERSONAL_KEY_ID = secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nGRAPHISTRY_PERSONAL_KEY_SECRET = secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n\n# Export OTel env vars for SDK auto-config (explicit v1 paths)\nos.environ[\"OTEL_EXPORTER_OTLP_PROTOCOL\"] = \"http/protobuf\"\nos.environ[\"OTEL_EXPORTER_OTLP_LOGS_ENDPOINT\"] = f\"{GRAFANA_OTLP_ENDPOINT}/v1/logs\"\nos.environ[\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\"] = f\"{GRAFANA_OTLP_ENDPOINT}/v1/traces\"\nos.environ[\"OTEL_EXPORTER_OTLP_METRICS_ENDPOINT\"] = f\"{GRAFANA_OTLP_ENDPOINT}/v1/metrics\"\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic%20{GRAFANA_OTLP_BASIC_B64}\"\nos.environ[\"OTEL_TRACES_EXPORTER\"] = \"otlp\"\nos.environ[\"OTEL_METRICS_EXPORTER\"] = \"otlp\"\n\n# Login/register\nlogin(HF_TOKEN)\n\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\",\n    personal_key_id=GRAPHISTRY_PERSONAL_KEY_ID,\n    personal_key_secret=GRAPHISTRY_PERSONAL_KEY_SECRET,\n)\n\nprint(\"Secrets configured and Graphistry registered\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:06:50.174880Z","iopub.execute_input":"2026-02-09T07:06:50.175186Z","iopub.status.idle":"2026-02-09T07:06:51.853869Z","shell.execute_reply.started":"2026-02-09T07:06:50.175161Z","shell.execute_reply":"2026-02-09T07:06:51.853122Z"}},"outputs":[{"name":"stdout","text":"Secrets configured and Graphistry registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================================== \n# Step 4: OpenTelemetry Setup (Grafana OTLP, Silent)\n# ============================================================================== \nimport logging\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n\n# Silence OTel logs\nlogging.getLogger().setLevel(logging.CRITICAL)\nlogging.getLogger(\"opentelemetry\").setLevel(logging.CRITICAL)\nlogging.getLogger(\"opentelemetry\").propagate = False\n\n# Shut down any previous providers\ntry:\n    trace.get_tracer_provider().shutdown()\nexcept Exception:\n    pass\ntry:\n    metrics.get_meter_provider().shutdown()\nexcept Exception:\n    pass\n\n# Define service resource with GPU context\nresource = Resource.create({\n    \"service.name\": \"llamatelemetry-inference\",\n    \"service.version\": \"0.1.0\",\n    \"service.instance.id\": \"kaggle-t4-worker-1\",\n    \"deployment.environment\": \"kaggle-notebook\",\n    \"host.name\": \"kaggle-gpu-0\",\n    \"gpu.model\": \"Tesla T4\",\n    \"gpu.memory.total\": 15360,  # MB\n    \"gpu.compute_capability\": \"7.5\",\n})\n\n# Tracer provider with Grafana OTLP\ntracer_provider = TracerProvider(resource=resource)\nspan_exporter = OTLPSpanExporter(\n    endpoint=f\"{GRAFANA_OTLP_ENDPOINT}/v1/traces\",\n    headers={\n        \"Authorization\": f\"Basic {GRAFANA_OTLP_BASIC_B64}\",\n        \"Content-Type\": \"application/x-protobuf\",\n    },\n)\ntracer_provider.add_span_processor(BatchSpanProcessor(span_exporter))\ntrace.set_tracer_provider(tracer_provider)\ntracer = trace.get_tracer(__name__)\n\n# Grafana sanity check\nwith tracer.start_as_current_span(\"grafana.sanity\") as span:\n    span.set_attribute(\"check\", \"ok\")\n\nprint(\"OpenTelemetry tracer configured\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:09:15.657764Z","iopub.execute_input":"2026-02-09T07:09:15.658422Z","iopub.status.idle":"2026-02-09T07:09:16.149523Z","shell.execute_reply.started":"2026-02-09T07:09:15.658387Z","shell.execute_reply":"2026-02-09T07:09:16.148901Z"}},"outputs":[{"name":"stdout","text":"OpenTelemetry tracer configured\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================== \n# Step 5: MeterProvider (Grafana OTLP, Silent)\n# ============================================================================== \nfrom opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nfrom opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter\n\nmetric_exporter = OTLPMetricExporter(\n    endpoint=f\"{GRAFANA_OTLP_ENDPOINT}/v1/metrics\",\n    headers={\"Authorization\": f\"Basic {GRAFANA_OTLP_BASIC_B64}\"},\n)\nmetric_reader = PeriodicExportingMetricReader(metric_exporter, export_interval_millis=5000)\n\nmeter_provider = MeterProvider(resource=resource, metric_readers=[metric_reader])\nmetrics.set_meter_provider(meter_provider)\nmeter = metrics.get_meter(__name__)\n\n# Custom instruments\nrequest_counter = meter.create_counter(\n    name=\"llm.requests.total\",\n    description=\"Total number of LLM requests\",\n    unit=\"1\",\n)\nlatency_histogram = meter.create_histogram(\n    name=\"llm.request.duration\",\n    description=\"LLM request latency\",\n    unit=\"ms\",\n)\ntoken_histogram = meter.create_histogram(\n    name=\"llm.tokens.total\",\n    description=\"Token usage per request\",\n    unit=\"{token}\",\n)\n\nprint(\"OpenTelemetry meter configured\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:09:27.137592Z","iopub.execute_input":"2026-02-09T07:09:27.139013Z","iopub.status.idle":"2026-02-09T07:09:27.266881Z","shell.execute_reply.started":"2026-02-09T07:09:27.138983Z","shell.execute_reply":"2026-02-09T07:09:27.266240Z"}},"outputs":[{"name":"stdout","text":"OpenTelemetry meter configured\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 2: Start Instrumented Server","metadata":{}},{"cell_type":"code","source":"# ============================================================================== \n# Step 6: Download GGUF Model\n# ============================================================================== \nfrom huggingface_hub import hf_hub_download\n\nrepo_id = \"bartowski/Qwen2.5-3B-Instruct-GGUF\"\nfilename = \"Qwen2.5-3B-Instruct-Q4_K_M.gguf\"\n\nmodel_path = hf_hub_download(repo_id=repo_id, filename=filename)\nprint(f\"Model downloaded: {model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:10:17.517566Z","iopub.execute_input":"2026-02-09T07:10:17.517909Z","iopub.status.idle":"2026-02-09T07:10:21.851376Z","shell.execute_reply.started":"2026-02-09T07:10:17.517883Z","shell.execute_reply":"2026-02-09T07:10:21.850740Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Qwen2.5-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43154cad69004d2b816e058c1783f404"}},"metadata":{}},{"name":"stdout","text":"Model downloaded: /root/.cache/huggingface/hub/models--bartowski--Qwen2.5-3B-Instruct-GGUF/snapshots/f302c64a2269a69fb27b2f9473b362f5bb8e78d8/Qwen2.5-3B-Instruct-Q4_K_M.gguf\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================== \n# Step 7: Start Server (GPU 0)\n# ============================================================================== \nfrom llamatelemetry.server import ServerManager\nimport torch\n\nprint(f\"Found {torch.cuda.device_count()} GPUs:\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n\nserver = ServerManager(server_url=\"http://127.0.0.1:8090\")\nserver.start_server(\n    model_path=model_path,\n    gpu_layers=99,\n    tensor_split=\"1.0,0.0\",  # GPU 0 only\n    flash_attn=1,\n    port=8090,\n    host=\"127.0.0.1\",\n    ctx_size=4096,\n    batch_size=512,\n)\n\nprint(\"Server running on http://127.0.0.1:8090\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:10:58.692407Z","iopub.execute_input":"2026-02-09T07:10:58.693132Z","iopub.status.idle":"2026-02-09T07:11:08.814655Z","shell.execute_reply.started":"2026-02-09T07:10:58.693105Z","shell.execute_reply":"2026-02-09T07:11:08.813896Z"}},"outputs":[{"name":"stdout","text":"Found 2 GPUs:\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Qwen2.5-3B-Instruct-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready...... âœ“ Ready in 3.0s\nServer running on http://127.0.0.1:8090\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================== \n# Step 8: Instrumented Inference (OTel -> Grafana)\n# ============================================================================== \nfrom llamatelemetry.api import LlamaCppClient\nfrom opentelemetry import trace\nimport time\n\nclass InstrumentedLLMClient:\n    def __init__(self, base_url: str, tracer):\n        self.client = LlamaCppClient(base_url)\n        self.tracer = tracer\n\n    def chat_completion(self, messages: list, **kwargs):\n        model = kwargs.get(\"model\", \"unknown\")\n        max_tokens = kwargs.get(\"max_tokens\", 100)\n        temperature = kwargs.get(\"temperature\", 0.7)\n\n        with self.tracer.start_as_current_span(\n            name=f\"llm.chat.{model}\",\n            kind=trace.SpanKind.CLIENT,\n        ) as span:\n            span.set_attribute(\"llm.system\", \"llama.cpp\")\n            span.set_attribute(\"llm.model\", model)\n            span.set_attribute(\"llm.request.max_tokens\", max_tokens)\n            span.set_attribute(\"llm.request.temperature\", temperature)\n            span.set_attribute(\"llm.request.messages\", len(messages))\n\n            start_time = time.time()\n            response = self.client.chat.completions.create(\n                messages=messages,\n                **kwargs\n            )\n            latency_ms = (time.time() - start_time) * 1000\n\n            finish_reason = response.choices[0].finish_reason\n            content = response.choices[0].message.content\n\n            span.set_attribute(\"llm.response.finish_reason\", finish_reason)\n            span.set_attribute(\"llm.response.length\", len(content))\n\n            # Emit OTel metrics\n            request_counter.add(1, attributes={\"model\": model, \"finish_reason\": finish_reason})\n            latency_histogram.record(latency_ms, attributes={\"model\": model})\n            token_histogram.record(len(content.split()), attributes={\"model\": model})\n\n            return response\n\nllm = InstrumentedLLMClient(\"http://127.0.0.1:8090\", tracer)\n\nresp = llm.chat_completion(\n    messages=[{\"role\": \"user\", \"content\": \"What is CUDA?\"}],\n    max_tokens=100,\n    temperature=0.7,\n)\nprint(resp.choices[0].message.content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:11:44.418010Z","iopub.execute_input":"2026-02-09T07:11:44.418765Z","iopub.status.idle":"2026-02-09T07:11:45.935919Z","shell.execute_reply.started":"2026-02-09T07:11:44.418735Z","shell.execute_reply":"2026-02-09T07:11:45.935178Z"}},"outputs":[{"name":"stdout","text":"CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by NVIDIA. It allows developers to use a GPU for general-purpose processing in addition to its primary role of accelerating graphics rendering.\n\n### Key Concepts:\n\n1. **GPU Computing**: GPUs are traditionally used for graphical computations, but CUDA enables them to be used for other types of computationally intensive tasks as well.\n\n2. **Parallel Processing**: By leveraging the parallel processing capabilities of GPUs, applications can achieve\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================== \n# Step 9: Metrics Collector (local, non-OTel) - reuse from notebook 15\n# ============================================================================== \nimport requests\nimport time\nimport re\nfrom typing import Dict, List\nfrom collections import defaultdict\nimport threading\nimport pandas as pd\n\nclass LlamaMetricsCollector:\n    def __init__(self, base_url: str = \"http://127.0.0.1:8090\"):\n        self.base_url = base_url\n        self.metrics_history = defaultdict(list)\n        self.slots_history = []\n        self.gpu_metrics_history = []\n        self.timestamps = []\n        self.running = False\n        self.lock = threading.Lock()\n\n    def parse_prometheus_metrics(self, text: str) -> Dict[str, float]:\n        metrics = {}\n        for line in text.split(\"\"):\n            if line.startswith(\"#\") or not line.strip():\n                continue\n            match = re.match(r\"(\\w+)\\s+([\\d.]+)\", line)\n            if match:\n                metric_name, value = match.groups()\n                metrics[metric_name] = float(value)\n        return metrics\n\n    def fetch_server_metrics(self) -> Dict[str, float]:\n        try:\n            response = requests.get(f\"{self.base_url}/metrics\", timeout=2)\n            if response.status_code == 200:\n                return self.parse_prometheus_metrics(response.text)\n        except Exception:\n            pass\n        return {}\n\n    def fetch_slots_info(self) -> List[Dict]:\n        try:\n            response = requests.get(f\"{self.base_url}/slots\", timeout=2)\n            if response.status_code == 200:\n                return response.json()\n        except Exception:\n            pass\n        return []\n\n    def fetch_gpu_metrics(self) -> Dict[str, float]:\n        try:\n            import pynvml\n            try:\n                pynvml.nvmlInit()\n            except:\n                pass\n            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n            temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n            power_draw = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000\n            return {\n                \"gpu_utilization\": utilization.gpu,\n                \"memory_utilization\": utilization.memory,\n                \"memory_used_mb\": memory_info.used / 1024**2,\n                \"memory_total_mb\": memory_info.total / 1024**2,\n                \"temperature_c\": temperature,\n                \"power_draw_w\": power_draw,\n            }\n        except Exception:\n            return {}\n\n    def collect_once(self):\n        timestamp = time.time()\n        server_metrics = self.fetch_server_metrics()\n        slots_info = self.fetch_slots_info()\n        gpu_metrics = self.fetch_gpu_metrics()\n\n        with self.lock:\n            self.timestamps.append(timestamp)\n            for key, value in server_metrics.items():\n                self.metrics_history[key].append(value)\n            self.slots_history.append({\n                \"timestamp\": timestamp,\n                \"slots\": slots_info,\n                \"num_processing\": sum(1 for s in slots_info if s.get(\"is_processing\", False)),\n                \"num_idle\": sum(1 for s in slots_info if not s.get(\"is_processing\", False)),\n            })\n            self.gpu_metrics_history.append({\"timestamp\": timestamp, **gpu_metrics})\n\n    def start_background_collection(self, interval: float = 1.0):\n        self.running = True\n        def collect_loop():\n            while self.running:\n                self.collect_once()\n                time.sleep(interval)\n        thread = threading.Thread(target=collect_loop, daemon=True)\n        thread.start()\n        print(f\"Started metrics collection (interval={interval}s)\")\n\n    def stop_background_collection(self):\n        self.running = False\n        print(\"Stopped metrics collection\")\n\n    def get_gpu_dataframe(self) -> pd.DataFrame:\n        with self.lock:\n            if not self.gpu_metrics_history:\n                return pd.DataFrame()\n            return pd.DataFrame(self.gpu_metrics_history)\n\ncollector = LlamaMetricsCollector()\ncollector.collect_once()\nprint(\"Metrics collector initialized\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:12:25.410140Z","iopub.execute_input":"2026-02-09T07:12:25.410493Z","iopub.status.idle":"2026-02-09T07:12:25.435984Z","shell.execute_reply.started":"2026-02-09T07:12:25.410447Z","shell.execute_reply":"2026-02-09T07:12:25.435260Z"}},"outputs":[{"name":"stdout","text":"Metrics collector initialized\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================================== \n# Step 10: Generate Continuous Inference Load (non-JS plotting)\n# ============================================================================== \nimport threading\n\nclass LoadGenerator:\n    def __init__(self, client, interval=1.0):\n        self.client = client\n        self.interval = interval\n        self.running = False\n        self.thread = None\n\n    def _loop(self):\n        prompts = [\n            \"Explain CUDA in simple terms.\",\n            \"What is GGUF?\",\n            \"Summarize LLM quantization.\",\n            \"Describe FlashAttention.\",\n        ]\n        i = 0\n        while self.running:\n            prompt = prompts[i % len(prompts)]\n            try:\n                self.client.chat_completion(\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    max_tokens=120,\n                    temperature=0.7,\n                )\n            except Exception:\n                pass\n            i += 1\n            time.sleep(self.interval)\n\n    def start(self):\n        self.running = True\n        self.thread = threading.Thread(target=self._loop, daemon=True)\n        self.thread.start()\n        print(\"Load generator started\")\n\n    def stop(self):\n        self.running = False\n        print(\"Load generator stopped\")\n\nload_gen = LoadGenerator(llm, interval=1.5)\ncollector.start_background_collection(interval=1.0)\nload_gen.start()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:12:59.211720Z","iopub.execute_input":"2026-02-09T07:12:59.212332Z","iopub.status.idle":"2026-02-09T07:12:59.223721Z","shell.execute_reply.started":"2026-02-09T07:12:59.212303Z","shell.execute_reply":"2026-02-09T07:12:59.222233Z"}},"outputs":[{"name":"stdout","text":"Started metrics collection (interval=1.0s)\nLoad generator started\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ============================================================================== \n# Step 11: Graphistry Visualization (GGUF Inference Architecture) - Non-OTel\n# ============================================================================== \nimport pandas as pd\nimport graphistry\n\n# Minimal GGUF inference architecture graph (non-OTel data)\n# Nodes: components, Edges: data flow\nnodes_df = pd.DataFrame([\n    {\"node_id\": \"input\", \"type\": \"io\"},\n    {\"node_id\": \"embed\", \"type\": \"embedding\"},\n    {\"node_id\": \"attn\", \"type\": \"attention\"},\n    {\"node_id\": \"ffn\", \"type\": \"ffn\"},\n    {\"node_id\": \"norm\", \"type\": \"norm\"},\n    {\"node_id\": \"output\", \"type\": \"io\"},\n])\n\nedges_df = pd.DataFrame([\n    {\"source\": \"input\", \"destination\": \"embed\", \"type\": \"flows\"},\n    {\"source\": \"embed\", \"destination\": \"attn\", \"type\": \"flows\"},\n    {\"source\": \"attn\", \"destination\": \"ffn\", \"type\": \"flows\"},\n    {\"source\": \"ffn\", \"destination\": \"norm\", \"type\": \"flows\"},\n    {\"source\": \"norm\", \"destination\": \"output\", \"type\": \"flows\"},\n])\n\n# Graphistry chart (non-OTel)\nplotter = graphistry.edges(edges_df, \"source\", \"destination\").nodes(nodes_df, \"node_id\")\nplotter = plotter.bind(point_color=\"type\", edge_color=\"type\")\nurl = plotter.plot(render=False)\nprint(f\"Graphistry URL (GGUF inference architecture): {url}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:13:06.595264Z","iopub.execute_input":"2026-02-09T07:13:06.596046Z","iopub.status.idle":"2026-02-09T07:13:08.091859Z","shell.execute_reply.started":"2026-02-09T07:13:06.596004Z","shell.execute_reply":"2026-02-09T07:13:08.091192Z"}},"outputs":[{"name":"stdout","text":"Graphistry URL (GGUF inference architecture): https://hub.graphistry.com/graph/graph.html?dataset=07d79781aae24bd7871576e76728b0ad&type=arrow&viztoken=ea308dd7-78c2-4550-9800-140652198d0f&usertag=4a131738-pygraphistry-0.50.6&splashAfter=1770621203&info=true\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ============================================================================== \n# Step 12: Stop Everything\n# ============================================================================== \nload_gen.stop()\ncollector.stop_background_collection()\nserver.stop_server()\nprint(\"Cleanup complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:20:25.040931Z","iopub.execute_input":"2026-02-09T07:20:25.041261Z","iopub.status.idle":"2026-02-09T07:20:35.309619Z","shell.execute_reply.started":"2026-02-09T07:20:25.041232Z","shell.execute_reply":"2026-02-09T07:20:35.308934Z"}},"outputs":[{"name":"stdout","text":"Load generator stopped\nStopped metrics collection\nCleanup complete\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}