{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN_2\")\nsecret_value_1 = user_secrets.get_secret(\"wand_api_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:17:13.168041Z","iopub.execute_input":"2026-02-28T10:17:13.168820Z","iopub.status.idle":"2026-02-28T10:17:13.303930Z","shell.execute_reply.started":"2026-02-28T10:17:13.168785Z","shell.execute_reply":"2026-02-28T10:17:13.303334Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import subprocess, textwrap, os, sys, time, json, re\nprint(subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout)\n\n# Quick structured view\nout = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free,utilization.gpu\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n).stdout.strip().splitlines()\n\nprint(\"\\nDetected GPUs:\")\nfor line in out:\n    print(\"  \", line)\n\nassert len(out) >= 2, \"This notebook expects Kaggle dual GPU (2x T4).\"\nprint(\"\\n‚úÖ Dual GPU available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:17:14.145205Z","iopub.execute_input":"2026-02-28T10:17:14.145980Z","iopub.status.idle":"2026-02-28T10:17:14.648411Z","shell.execute_reply.started":"2026-02-28T10:17:14.145947Z","shell.execute_reply":"2026-02-28T10:17:14.647692Z"}},"outputs":[{"name":"stdout","text":"Sat Feb 28 10:17:14 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   46C    P8             13W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   45C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n\n\nDetected GPUs:\n   0, Tesla T4, 15360 MiB, 14910 MiB, 0 %\n   1, Tesla T4, 15360 MiB, 14910 MiB, 0 %\n\n‚úÖ Dual GPU available.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# llamatelemetry v0.1.0\n!pip install -q https://github.com/llamatelemetry/llamatelemetry/releases/download/v0.1.0/llamatelemetry-v0.1.0-source.tar.gz\n\n# W&B + HF downloader + metrics helpers\n!pip install -q wandb huggingface_hub requests pandas pynvml psutil\n\nimport wandb, pandas as pd, requests, psutil\nprint(\"‚úÖ Installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:17:15.408036Z","iopub.execute_input":"2026-02-28T10:17:15.408729Z","iopub.status.idle":"2026-02-28T10:17:28.769558Z","shell.execute_reply.started":"2026-02-28T10:17:15.408695Z","shell.execute_reply":"2026-02-28T10:17:28.768784Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m798.4/798.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n‚úÖ Installed.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nsecrets = UserSecretsClient()\n\n# W&B\nos.environ[\"wand_api_token\"] = secrets.get_secret(\"wand_api_token\")\n\n# HF (for GGUF download)\nHF_TOKEN = secrets.get_secret(\"HF_TOKEN_2\")\nlogin(token=HF_TOKEN)\n\nprint(\"‚úÖ Secrets loaded and HF login done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:17:33.519876Z","iopub.execute_input":"2026-02-28T10:17:33.520690Z","iopub.status.idle":"2026-02-28T10:17:33.818100Z","shell.execute_reply.started":"2026-02-28T10:17:33.520654Z","shell.execute_reply":"2026-02-28T10:17:33.817339Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Secrets loaded and HF login done.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import os, time\n\nWANDB_PROJECT = os.environ.get(\"WANDB_PROJECT\", \"llamatelemetry-kaggle\")\nWANDB_ENTITY  = os.environ.get(\"WANDB_ENTITY\", None)  # optional\n\nrun = wandb.init(\n    project=WANDB_PROJECT,\n    entity=WANDB_ENTITY,\n    name=f\"kaggle-dual-t4-llamatelemetry-{int(time.time())}\",\n    config={\n        \"stack\": \"llamatelemetry + llama.cpp + wandb\",\n        \"hardware\": \"kaggle dual t4\",\n    },\n)\n\nprint(\"‚úÖ W&B run:\", run.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:17:35.016099Z","iopub.execute_input":"2026-02-28T10:17:35.016443Z","iopub.status.idle":"2026-02-28T10:17:41.507719Z","shell.execute_reply.started":"2026-02-28T10:17:35.016415Z","shell.execute_reply":"2026-02-28T10:17:41.507106Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.24.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20260228_101735-r8cdhnvc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/waqasm86-aprilventure/llamatelemetry-kaggle/runs/r8cdhnvc' target=\"_blank\">kaggle-dual-t4-llamatelemetry-1772273855</a></strong> to <a href='https://wandb.ai/waqasm86-aprilventure/llamatelemetry-kaggle' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/waqasm86-aprilventure/llamatelemetry-kaggle' target=\"_blank\">https://wandb.ai/waqasm86-aprilventure/llamatelemetry-kaggle</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/waqasm86-aprilventure/llamatelemetry-kaggle/runs/r8cdhnvc' target=\"_blank\">https://wandb.ai/waqasm86-aprilventure/llamatelemetry-kaggle/runs/r8cdhnvc</a>"},"metadata":{}},{"name":"stdout","text":"‚úÖ W&B run: kaggle-dual-t4-llamatelemetry-1772273855\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\n\nrepo_id = \"bartowski/Qwen2.5-3B-Instruct-GGUF\"\nfilename = \"Qwen2.5-3B-Instruct-Q4_K_M.gguf\"\n\nmodel_path = hf_hub_download(repo_id=repo_id, filename=filename)\nprint(\"‚úÖ Model:\", model_path)\n\n# Log config to W&B\nwandb.config.update({\"repo_id\": repo_id, \"filename\": filename}, allow_val_change=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:17:41.508833Z","iopub.execute_input":"2026-02-28T10:17:41.509075Z","iopub.status.idle":"2026-02-28T10:17:41.685751Z","shell.execute_reply.started":"2026-02-28T10:17:41.509053Z","shell.execute_reply":"2026-02-28T10:17:41.685194Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model: /root/.cache/huggingface/hub/models--bartowski--Qwen2.5-3B-Instruct-GGUF/snapshots/f302c64a2269a69fb27b2f9473b362f5bb8e78d8/Qwen2.5-3B-Instruct-Q4_K_M.gguf\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nfrom llamatelemetry.server import ServerManager\n\nprint(f\"torch.cuda.device_count() = {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n\nBASE_URL = \"http://127.0.0.1:8090\"\n\nserver = ServerManager(server_url=BASE_URL)\n\n# You can tweak ctx_size/batch_size for T4 stability\nserver.start_server(\n    model_path=model_path,\n    gpu_layers=99,\n    tensor_split=\"0.5,0.5\",   # <-- Dual GPU\n    flash_attn=1,\n    port=8090,\n    host=\"127.0.0.1\",\n    ctx_size=4096,\n    batch_size=256,\n)\n\nprint(\"‚úÖ Server running:\", BASE_URL)\n\n# Log server config\nwandb.config.update({\n    \"server_url\": BASE_URL,\n    \"gpu_layers\": 99,\n    \"tensor_split\": \"0.5,0.5\",\n    \"ctx_size\": 4096,\n    \"batch_size\": 256,\n}, allow_val_change=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:17:43.491873Z","iopub.execute_input":"2026-02-28T10:17:43.492609Z","iopub.status.idle":"2026-02-28T10:17:45.559579Z","shell.execute_reply.started":"2026-02-28T10:17:43.492580Z","shell.execute_reply":"2026-02-28T10:17:45.558844Z"}},"outputs":[{"name":"stdout","text":"torch.cuda.device_count() = 2\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Qwen2.5-3B-Instruct-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready..... ‚úì Ready in 2.0s\n‚úÖ Server running: http://127.0.0.1:8090\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from llamatelemetry.api import LlamaCppClient\nimport time\n\nclient = LlamaCppClient(BASE_URL)\n\nt0 = time.time()\nresp = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Say hello in one short sentence.\"}],\n    max_tokens=32,\n    temperature=0.2,\n)\nlat_ms = (time.time() - t0) * 1000\n\ntext = resp.choices[0].message.content\nprint(\"Response:\", text)\nprint(f\"Latency: {lat_ms:.1f} ms\")\n\nwandb.log({\"smoke/latency_ms\": lat_ms, \"smoke/ok\": 1})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:18:20.196121Z","iopub.execute_input":"2026-02-28T10:18:20.196935Z","iopub.status.idle":"2026-02-28T10:18:20.425382Z","shell.execute_reply.started":"2026-02-28T10:18:20.196904Z","shell.execute_reply":"2026-02-28T10:18:20.424556Z"}},"outputs":[{"name":"stdout","text":"Response: Hello! How can I assist you today?\nLatency: 219.4 ms\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import threading\nfrom collections import defaultdict\nimport pynvml\n\nclass LlamaMetricsCollector:\n    \"\"\"\n    Collects:\n      - /metrics (Prometheus text)\n      - /slots (JSON)\n      - NVML GPU metrics for GPU0 and GPU1\n      - host CPU/RAM\n    \"\"\"\n    def __init__(self, base_url: str):\n        self.base_url = base_url.rstrip(\"/\")\n        self.metrics_history = defaultdict(list)\n        self.timestamps = []\n        self.slots_history = []\n        self.gpu_history = []\n        self.host_history = []\n        self.running = False\n        self.lock = threading.Lock()\n        try:\n            pynvml.nvmlInit()\n        except Exception as e:\n            print(\"NVML init warning:\", e)\n\n    def _parse_prom_text(self, text: str):\n        metrics = {}\n        for line in text.split(\"\\n\"):\n            if line.startswith(\"#\") or not line.strip():\n                continue\n            # Handle: metric_name value\n            m = re.match(r\"^([a-zA-Z_:][a-zA-Z0-9_:]*)\\s+([-+eE0-9.]+)$\", line.strip())\n            if m:\n                k, v = m.group(1), float(m.group(2))\n                metrics[k] = v\n        return metrics\n\n    def fetch_server_metrics(self):\n        try:\n            r = requests.get(f\"{self.base_url}/metrics\", timeout=2)\n            if r.status_code == 200:\n                return self._parse_prom_text(r.text)\n        except Exception:\n            pass\n        return {}\n\n    def fetch_slots(self):\n        try:\n            r = requests.get(f\"{self.base_url}/slots\", timeout=2)\n            if r.status_code == 200:\n                return r.json()\n        except Exception:\n            pass\n        return []\n\n    def fetch_gpu(self, gpu_index: int):\n        try:\n            h = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n            util = pynvml.nvmlDeviceGetUtilizationRates(h)\n            mem  = pynvml.nvmlDeviceGetMemoryInfo(h)\n            temp = pynvml.nvmlDeviceGetTemperature(h, pynvml.NVML_TEMPERATURE_GPU)\n            pwr  = pynvml.nvmlDeviceGetPowerUsage(h) / 1000.0\n            return {\n                \"gpu_index\": gpu_index,\n                \"gpu_util\": float(util.gpu),\n                \"mem_util\": float(util.memory),\n                \"mem_used_mb\": float(mem.used / 1024**2),\n                \"mem_total_mb\": float(mem.total / 1024**2),\n                \"temp_c\": float(temp),\n                \"power_w\": float(pwr),\n            }\n        except Exception:\n            return {\"gpu_index\": gpu_index}\n\n    def fetch_host(self):\n        return {\n            \"cpu_percent\": float(psutil.cpu_percent(interval=None)),\n            \"ram_used_gb\": float(psutil.virtual_memory().used / 1024**3),\n            \"ram_total_gb\": float(psutil.virtual_memory().total / 1024**3),\n        }\n\n    def collect_once(self):\n        ts = time.time()\n        m = self.fetch_server_metrics()\n        slots = self.fetch_slots()\n        g0 = self.fetch_gpu(0)\n        g1 = self.fetch_gpu(1)\n        host = self.fetch_host()\n\n        with self.lock:\n            self.timestamps.append(ts)\n            for k, v in m.items():\n                self.metrics_history[k].append(v)\n            self.slots_history.append({\n                \"ts\": ts,\n                \"num_slots\": len(slots),\n                \"num_processing\": sum(1 for s in slots if s.get(\"is_processing\", False)),\n            })\n            self.gpu_history.append({\"ts\": ts, \"gpu0\": g0, \"gpu1\": g1})\n            self.host_history.append({\"ts\": ts, **host})\n\n        return ts, m, slots, g0, g1, host\n\n    def start(self, interval_s=1.0):\n        self.running = True\n        def loop():\n            while self.running:\n                self.collect_once()\n                time.sleep(interval_s)\n        t = threading.Thread(target=loop, daemon=True)\n        t.start()\n        print(f\"‚úÖ Collector started (interval={interval_s}s)\")\n\n    def stop(self):\n        self.running = False\n        print(\"üõë Collector stopped\")\n\ncollector = LlamaMetricsCollector(BASE_URL)\ncollector.collect_once()\nprint(\"‚úÖ Collector initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:18:22.797996Z","iopub.execute_input":"2026-02-28T10:18:22.798763Z","iopub.status.idle":"2026-02-28T10:18:22.828966Z","shell.execute_reply.started":"2026-02-28T10:18:22.798719Z","shell.execute_reply":"2026-02-28T10:18:22.828213Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Collector initialized\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"#Do not run this code\n\n\"\"\"\ndef pick_metrics(prom: dict):\n    \"\"\"\n    Different llama.cpp builds expose different metric names.\n    We log a subset if they exist.\n    \"\"\"\n    keys = [\n        \"llama_requests_total\",\n        \"llama_tokens_total\",\n        \"llama_tokens_per_second\",\n        \"llama_prompt_tokens_total\",\n        \"llama_generated_tokens_total\",\n        \"process_resident_memory_bytes\",\n    ]\n    out = {}\n    for k in keys:\n        if k in prom:\n            out[f\"server/{k}\"] = prom[k]\n    return out\n\ndef wandb_metrics_loop(interval_s=2.0):\n    step = 0\n    while True:\n        ts, prom, slots, g0, g1, host = collector.collect_once()\n\n        log = {}\n        # GPU0\n        log.update({\n            \"gpu0/util\": g0.get(\"gpu_util\", None),\n            \"gpu0/mem_used_mb\": g0.get(\"mem_used_mb\", None),\n            \"gpu0/mem_total_mb\": g0.get(\"mem_total_mb\", None),\n            \"gpu0/temp_c\": g0.get(\"temp_c\", None),\n            \"gpu0/power_w\": g0.get(\"power_w\", None),\n        })\n        # GPU1\n        log.update({\n            \"gpu1/util\": g1.get(\"gpu_util\", None),\n            \"gpu1/mem_used_mb\": g1.get(\"mem_used_mb\", None),\n            \"gpu1/mem_total_mb\": g1.get(\"mem_total_mb\", None),\n            \"gpu1/temp_c\": g1.get(\"temp_c\", None),\n            \"gpu1/power_w\": g1.get(\"power_w\", None),\n        })\n        # Host\n        log.update({\n            \"host/cpu_percent\": host[\"cpu_percent\"],\n            \"host/ram_used_gb\": host[\"ram_used_gb\"],\n            \"host/ram_total_gb\": host[\"ram_total_gb\"],\n        })\n        # Slots\n        log.update({\n            \"server/slots_num\": len(slots),\n            \"server/slots_processing\": sum(1 for s in slots if s.get(\"is_processing\", False)),\n        })\n        # Optional server metrics\n        log.update(pick_metrics(prom))\n\n        wandb.log(log, step=step)\n        step += 1\n        time.sleep(interval_s)\n\n# Start collector + W&B streamers\ncollector.start(interval_s=1.0)\n\nmetrics_thread = threading.Thread(target=wandb_metrics_loop, kwargs={\"interval_s\": 2.0}, daemon=True)\nmetrics_thread.start()\n\nprint(\"‚úÖ W&B metrics streaming started\")\n\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#step 9 code\n\nimport threading, time\n\nwandb_stop_event = threading.Event()\n\ndef wandb_metrics_loop(interval_s=2.0, stop_event=None):\n    step = 0\n    while True:\n        if stop_event is not None and stop_event.is_set():\n            break\n\n        ts, prom, slots, g0, g1, host = collector.collect_once()\n\n        log = {}\n        # GPU0\n        log.update({\n            \"gpu0/util\": g0.get(\"gpu_util\", None),\n            \"gpu0/mem_used_mb\": g0.get(\"mem_used_mb\", None),\n            \"gpu0/mem_total_mb\": g0.get(\"mem_total_mb\", None),\n            \"gpu0/temp_c\": g0.get(\"temp_c\", None),\n            \"gpu0/power_w\": g0.get(\"power_w\", None),\n        })\n        # GPU1\n        log.update({\n            \"gpu1/util\": g1.get(\"gpu_util\", None),\n            \"gpu1/mem_used_mb\": g1.get(\"mem_used_mb\", None),\n            \"gpu1/mem_total_mb\": g1.get(\"mem_total_mb\", None),\n            \"gpu1/temp_c\": g1.get(\"temp_c\", None),\n            \"gpu1/power_w\": g1.get(\"power_w\", None),\n        })\n        # Host\n        log.update({\n            \"host/cpu_percent\": host[\"cpu_percent\"],\n            \"host/ram_used_gb\": host[\"ram_used_gb\"],\n            \"host/ram_total_gb\": host[\"ram_total_gb\"],\n        })\n        # Slots\n        log.update({\n            \"server/slots_num\": len(slots),\n            \"server/slots_processing\": sum(1 for s in slots if s.get(\"is_processing\", False)),\n        })\n        # Optional server metrics\n        log.update(pick_metrics(prom))\n\n        # Important: Only log if run is still active\n        if wandb.run is not None:\n            wandb.log(log, step=step)\n\n        step += 1\n        time.sleep(interval_s)\n\ncollector.start(interval_s=1.0)\n\nmetrics_thread = threading.Thread(\n    target=wandb_metrics_loop,\n    kwargs={\"interval_s\": 2.0, \"stop_event\": wandb_stop_event},\n    daemon=True\n)\nmetrics_thread.start()\n\nprint(\"‚úÖ W&B metrics streaming started (with stop event)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:18:29.585428Z","iopub.execute_input":"2026-02-28T10:18:29.586153Z","iopub.status.idle":"2026-02-28T10:18:29.606067Z","shell.execute_reply.started":"2026-02-28T10:18:29.586122Z","shell.execute_reply":"2026-02-28T10:18:29.605217Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Collector started (interval=1.0s)\n‚úÖ W&B metrics streaming started (with stop event)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 46. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def estimate_tokens(s: str) -> int:\n    # crude token proxy (good enough for trend charts)\n    return max(1, len(s.split()))\n\ndef chat_and_log(prompt: str, max_tokens=128, temperature=0.7):\n    t0 = time.time()\n    resp = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=max_tokens,\n        temperature=temperature,\n    )\n    lat_ms = (time.time() - t0) * 1000.0\n    text = resp.choices[0].message.content\n\n    tok_out = estimate_tokens(text)\n    wandb.log({\n        \"infer/latency_ms\": lat_ms,\n        \"infer/tokens_out_est\": tok_out,\n        \"infer/tokens_per_sec_est\": (tok_out / (lat_ms/1000.0)) if lat_ms > 0 else None,\n    })\n    return text, lat_ms\n\n# Quick test\ntxt, lat = chat_and_log(\"Explain what GGUF is in 2 bullet points.\", max_tokens=80, temperature=0.3)\nprint(txt)\nprint(\"Latency(ms):\", lat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:18:37.282511Z","iopub.execute_input":"2026-02-28T10:18:37.282981Z","iopub.status.idle":"2026-02-28T10:18:38.549543Z","shell.execute_reply.started":"2026-02-28T10:18:37.282954Z","shell.execute_reply":"2026-02-28T10:18:38.548737Z"}},"outputs":[{"name":"stdout","text":"- GGUF refers to the model format and optimized parameters used for the Generation-Guided Unified Format, which is a pre-trained language model designed for fine-tuning with generation guidance.\n- It was created by Anthropic as part of their efforts to standardize large language model formats to facilitate research and development in AI.\nLatency(ms): 1256.530523300171\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 47. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 48. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 49. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 50. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 51. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"class LoadGenerator:\n    def __init__(self, interval_s=1.5):\n        self.interval_s = interval_s\n        self.running = False\n        self.thread = None\n        self.prompts = [\n            \"Explain CUDA in simple terms.\",\n            \"What is GGUF? Keep it short.\",\n            \"Summarize LLM quantization in 3 lines.\",\n            \"Describe FlashAttention at a high level.\",\n        ]\n\n    def _loop(self):\n        i = 0\n        while self.running:\n            prompt = self.prompts[i % len(self.prompts)]\n            try:\n                chat_and_log(prompt, max_tokens=120, temperature=0.7)\n            except Exception as e:\n                wandb.log({\"infer/errors\": 1})\n            i += 1\n            time.sleep(self.interval_s)\n\n    def start(self):\n        self.running = True\n        self.thread = threading.Thread(target=self._loop, daemon=True)\n        self.thread.start()\n        print(\"‚úÖ Load generator started\")\n\n    def stop(self):\n        self.running = False\n        print(\"üõë Load generator stopped\")\n\nload_gen = LoadGenerator(interval_s=1.5)\nload_gen.start()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:18:44.653655Z","iopub.execute_input":"2026-02-28T10:18:44.653969Z","iopub.status.idle":"2026-02-28T10:18:44.664330Z","shell.execute_reply.started":"2026-02-28T10:18:44.653942Z","shell.execute_reply":"2026-02-28T10:18:44.663593Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Load generator started\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"LATENCY_ALERT_MS = 5000  # tune this\n\n# Run a few prompts and alert if slow\nfor p in [\"Write a short haiku about GPUs.\", \"Give me 5 tips to optimize inference on T4.\"]:\n    txt, lat = chat_and_log(p, max_tokens=120, temperature=0.6)\n    print(f\"\\nPrompt: {p}\\nLatency: {lat:.0f} ms\\n{txt}\")\n    if lat > LATENCY_ALERT_MS:\n        wandb.alert(\n            title=\"High inference latency\",\n            text=f\"Latency {lat:.0f}ms exceeded {LATENCY_ALERT_MS}ms on tensor_split={wandb.config.get('tensor_split')}\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:18:47.398116Z","iopub.execute_input":"2026-02-28T10:18:47.398451Z","iopub.status.idle":"2026-02-28T10:18:49.351902Z","shell.execute_reply.started":"2026-02-28T10:18:47.398423Z","shell.execute_reply":"2026-02-28T10:18:49.350603Z"}},"outputs":[{"name":"stdout","text":"\nPrompt: Write a short haiku about GPUs.\nLatency: 256 ms\nGigantic cores hum,\nGPU speeds through calculations,\nPower in circuits.\n\nPrompt: Give me 5 tips to optimize inference on T4.\nLatency: 1686 ms\nSure! Optimizing inference for the NVIDIA T4 GPU involves several strategies that can help improve performance and reduce latency. Here are five key tips:\n\n1. **Batch Size Optimization**:\n   - Adjust your batch size to achieve optimal throughput without compromising model accuracy. Generally, larger batches can lead to better utilization of the GPU memory but may increase the risk of out-of-memory errors if not managed properly.\n   - Experiment with different batch sizes and monitor the inference time and model accuracy.\n\n2. **Model Pruning and Quantization**:\n   - Perform pruning and quantization on your model to reduce its\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Stop load generator\ntry:\n    load_gen.stop()\nexcept Exception as e:\n    print(\"Load gen stop warning:\", e)\n\n# Stop W&B metrics thread\ntry:\n    wandb_stop_event.set()\n    if metrics_thread.is_alive():\n        metrics_thread.join(timeout=5)\nexcept Exception as e:\n    print(\"Metrics thread stop warning:\", e)\n\n# Stop collector\ntry:\n    collector.stop()\nexcept Exception as e:\n    print(\"Collector stop warning:\", e)\n\n# Stop server\ntry:\n    server.stop_server()\nexcept Exception as e:\n    print(\"Server stop warning:\", e)\n\n# Finish W&B last\ntry:\n    wandb.finish()\nexcept Exception as e:\n    print(\"wandb.finish warning:\", e)\n\nprint(\"‚úÖ Cleanup complete (clean thread shutdown)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T10:22:24.137332Z","iopub.execute_input":"2026-02-28T10:22:24.137708Z","iopub.status.idle":"2026-02-28T10:22:25.496315Z","shell.execute_reply.started":"2026-02-28T10:22:24.137679Z","shell.execute_reply":"2026-02-28T10:22:25.495598Z"}},"outputs":[{"name":"stdout","text":"üõë Load generator stopped\nüõë Collector stopped\n‚úÖ Cleanup complete (clean thread shutdown)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}