{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook 16: End-to-End Production Observability Stack\n\nUpdated to match notebook 14 (OTel -> Grafana) and notebook 15 runtime flow.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 1: Verify Dual GPU Environment\n# ==============================================================================\n\nimport subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"ğŸ” SPLIT-GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\ngpus = result.stdout.strip().split('\\n')\nprint(f\"\\nğŸ“Š Detected {len(gpus)} GPU(s):\")\nfor gpu in gpus:\n    print(f\"   {gpu}\")\n\nif len(gpus) >= 2:\n    print(\"\\nâœ… Dual T4 ready for split-GPU operation!\")\n    print(\"   GPU 0 â†’ llama-server (GGUF model inference)\")\n    print(\"   GPU 1 â†’ RAPIDS/Graphistry (architecture visualization)\")\nelse:\n    print(\"\\nâš ï¸ Need 2 GPUs for split operation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:20:07.255171Z","iopub.execute_input":"2026-02-09T08:20:07.255501Z","iopub.status.idle":"2026-02-09T08:20:07.305481Z","shell.execute_reply.started":"2026-02-09T08:20:07.255477Z","shell.execute_reply":"2026-02-09T08:20:07.304685Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ” SPLIT-GPU ENVIRONMENT CHECK\n======================================================================\n\nğŸ“Š Detected 2 GPU(s):\n   0, Tesla T4, 15360 MiB, 14913 MiB\n   1, Tesla T4, 15360 MiB, 14913 MiB\n\nâœ… Dual T4 ready for split-GPU operation!\n   GPU 0 â†’ llama-server (GGUF model inference)\n   GPU 1 â†’ RAPIDS/Graphistry (architecture visualization)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==============================================================================\n# Step 2: Install llamatelemetry v0.1.0\n# ==============================================================================\nprint(\"ğŸ“¦ Installing dependencies...\")\n\n# Install llamatelemetry v0.1.0\n!pip install -q https://github.com/llamatelemetry/llamatelemetry/releases/download/v0.1.0/llamatelemetry-v0.1.0-source.tar.gz\n#!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\n# Install cuGraph for GPU-accelerated graph algorithms\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\" \"cudf-cu12==25.6.*\"\n\n# Install Graphistry for visualization\n!pip install -q \"graphistry[ai]\"\n\n# Install additional utilities\n!pip install -q pyarrow pandas numpy scipy huggingface_hub\n\n# Verify installations\nimport llamatelemetry\nprint(f\"\\nâœ… llamatelemetry {llamatelemetry.__version__} installed\")\n\ntry:\n    import cudf, cugraph\n    print(f\"âœ… cuDF {cudf.__version__}\")\n    print(f\"âœ… cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"âœ… Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Graphistry: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:20:07.497925Z","iopub.execute_input":"2026-02-09T08:20:07.498183Z","iopub.status.idle":"2026-02-09T08:21:49.716885Z","shell.execute_reply.started":"2026-02-09T08:20:07.498160Z","shell.execute_reply":"2026-02-09T08:21:49.715902Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Installing dependencies...\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m763.5/763.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nğŸ¯ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2Ã— T4 Multi-GPU\n======================================================================\n\nğŸ® GPU Detected: Tesla T4 (Compute 7.5)\n  âœ… Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nğŸŒ Platform: Colab\n\nğŸ“¦ Downloading Kaggle 2Ã— T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\nâ¡ï¸  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nğŸ“¥ Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(â€¦):   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bf428ee204549239cc7ffb072b5a619"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Verifying SHA256 checksum...\n   âœ… Checksum verified\nğŸ“¦ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\nâœ… Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\nâœ… Binaries installed successfully!\n\n\nâœ… llamatelemetry 0.1.0 installed\nâœ… cuDF 25.06.00\nâœ… cuGraph 25.06.00\nâœ… Graphistry 0.50.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# First, downgrade to compatible versions\n!pip install -q \\\n  opentelemetry-api==1.37.0 \\\n  opentelemetry-sdk==1.37.0 \\\n  opentelemetry-proto==1.37.0 \\\n  opentelemetry-exporter-otlp-proto-common==1.37.0 \\\n  opentelemetry-exporter-otlp-proto-grpc==1.37.0 \\\n  rich==13.9.4 \\\n  --upgrade-strategy=only-if-needed\n\n# Also install the missing bigquery storage package\n!pip install -q google-cloud-bigquery-storage==2.31.0 --upgrade-strategy=only-if-needed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:21:49.718441Z","iopub.execute_input":"2026-02-09T08:21:49.719077Z","iopub.status.idle":"2026-02-09T08:21:57.776092Z","shell.execute_reply.started":"2026-02-09T08:21:49.719049Z","shell.execute_reply":"2026-02-09T08:21:57.775373Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m256.5/256.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==============================================================================\n# Step 3: Setup Secrets (Kaggle Secrets)\n# ==============================================================================\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\nsecrets = UserSecretsClient()\n\n# Grafana OTLP\nGRAFANA_OTLP_ENDPOINT = secrets.get_secret(\"GRAFANA_OTLP_ENDPOINT\").rstrip(\"/\")\nGRAFANA_OTLP_BASIC_B64 = secrets.get_secret(\"GRAFANA_OTLP_BASIC_B64\")\nGRAFANA_OTLP_INSTANCE_ID = secrets.get_secret(\"GRAFANA_OTLP_INSTANCE_ID\")\nGRAFANA_OTLP_TOKEN = secrets.get_secret(\"GRAFANA_OTLP_TOKEN\")\n\n# HuggingFace\nHF_TOKEN = secrets.get_secret(\"HF_TOKEN_2\")\n\n# Graphistry\n#GRAPHISTRY_USERNAME = secrets.get_secret(\"Graphistry_Username\")\nGRAPHISTRY_PERSONAL_KEY_ID = secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nGRAPHISTRY_PERSONAL_KEY_SECRET = secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n\n# Export OTel env vars for SDK auto-config (explicit v1 paths)\nos.environ[\"OTEL_EXPORTER_OTLP_PROTOCOL\"] = \"http/protobuf\"\nos.environ[\"OTEL_EXPORTER_OTLP_LOGS_ENDPOINT\"] = f\"{GRAFANA_OTLP_ENDPOINT}/v1/logs\"\nos.environ[\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\"] = f\"{GRAFANA_OTLP_ENDPOINT}/v1/traces\"\nos.environ[\"OTEL_EXPORTER_OTLP_METRICS_ENDPOINT\"] = f\"{GRAFANA_OTLP_ENDPOINT}/v1/metrics\"\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic%20{GRAFANA_OTLP_BASIC_B64}\"\nos.environ[\"OTEL_TRACES_EXPORTER\"] = \"otlp\"\nos.environ[\"OTEL_METRICS_EXPORTER\"] = \"otlp\"\n\n# Login/register\nfrom huggingface_hub import login\nimport graphistry\n\nlogin(HF_TOKEN)\n\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\",\n    personal_key_id=GRAPHISTRY_PERSONAL_KEY_ID,\n    personal_key_secret=GRAPHISTRY_PERSONAL_KEY_SECRET,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:22:04.663141Z","iopub.execute_input":"2026-02-09T08:22:04.663800Z","iopub.status.idle":"2026-02-09T08:22:05.927478Z","shell.execute_reply.started":"2026-02-09T08:22:04.663770Z","shell.execute_reply":"2026-02-09T08:22:05.926647Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<graphistry.pygraphistry.GraphistryClient at 0x78e3180c27e0>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"### **Step 4: OpenTelemetry Setup (Grafana OTLP, Silent)**\n\nimport logging\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n\n# Hard-silence OTel logs\nlogging.getLogger().setLevel(logging.CRITICAL)\nlogging.getLogger(\"opentelemetry\").setLevel(logging.CRITICAL)\nlogging.getLogger(\"opentelemetry\").propagate = False\n\n# Shut down any previous providers to stop old exporters\ntry:\n    trace.get_tracer_provider().shutdown()\nexcept Exception:\n    pass\ntry:\n    metrics.get_meter_provider().shutdown()\nexcept Exception:\n    pass\n\n# Normalize endpoint\nGRAFANA_OTLP_ENDPOINT = GRAFANA_OTLP_ENDPOINT.rstrip(\"/\")\n\n# Define service resource with GPU context\nresource = Resource.create({\n    \"service.name\": \"llamatelemetry-inference\",\n    \"service.version\": \"0.1.0\",\n    \"service.instance.id\": \"kaggle-t4-worker-1\",\n    \"deployment.environment\": \"kaggle-notebook\",\n    \"host.name\": \"kaggle-gpu-0\",\n    \"gpu.model\": \"Tesla T4\",\n    \"gpu.memory.total\": 15360,  # MB\n    \"gpu.compute_capability\": \"7.5\",\n})\n\n# Create tracer provider with resource (NO console exporter)\ntracer_provider = TracerProvider(resource=resource)\n\n# OTLP exporter to Grafana (explicit traces endpoint)\nspan_exporter = OTLPSpanExporter(\n    endpoint=f\"{GRAFANA_OTLP_ENDPOINT}/v1/traces\",\n    headers={\n        \"Authorization\": f\"Basic {GRAFANA_OTLP_BASIC_B64}\",\n        \"Content-Type\": \"application/x-protobuf\",\n    },\n)\ntracer_provider.add_span_processor(BatchSpanProcessor(span_exporter))\n\ntrace.set_tracer_provider(tracer_provider)\ntracer = trace.get_tracer(__name__)\n\n# Grafana sanity check (silent)\nwith tracer.start_as_current_span(\"grafana.sanity\") as span:\n    span.set_attribute(\"check\", \"ok\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:22:08.028037Z","iopub.execute_input":"2026-02-09T08:22:08.028720Z","iopub.status.idle":"2026-02-09T08:22:08.294771Z","shell.execute_reply.started":"2026-02-09T08:22:08.028691Z","shell.execute_reply":"2026-02-09T08:22:08.293963Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# **Setup MeterProvider (Grafana OTLP, Silent)**\n\nfrom opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nfrom opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter\n\n# OTLP metric exporter to Grafana (silent)\nmetric_exporter = OTLPMetricExporter(\n    endpoint=f\"{GRAFANA_OTLP_ENDPOINT}/v1/metrics\",\n    headers={\"Authorization\": f\"Basic {GRAFANA_OTLP_BASIC_B64}\"},\n)\nmetric_reader = PeriodicExportingMetricReader(\n    metric_exporter,\n    export_interval_millis=5000,\n)\n\nmeter_provider = MeterProvider(resource=resource, metric_readers=[metric_reader])\nmetrics.set_meter_provider(meter_provider)\nmeter = metrics.get_meter(__name__)\n\n# Create custom instruments\nrequest_counter = meter.create_counter(\n    name=\"llm.requests.total\",\n    description=\"Total number of LLM requests\",\n    unit=\"1\",\n)\n\nlatency_histogram = meter.create_histogram(\n    name=\"llm.request.duration\",\n    description=\"LLM request latency\",\n    unit=\"ms\",\n)\n\ntoken_histogram = meter.create_histogram(\n    name=\"llm.tokens.total\",\n    description=\"Token usage per request\",\n    unit=\"{token}\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:22:11.568922Z","iopub.execute_input":"2026-02-09T08:22:11.569912Z","iopub.status.idle":"2026-02-09T08:22:11.654648Z","shell.execute_reply.started":"2026-02-09T08:22:11.569879Z","shell.execute_reply":"2026-02-09T08:22:11.654081Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ============================================================================== \n# Step 5: Download GGUF Model\n# ============================================================================== \nfrom huggingface_hub import hf_hub_download\n\nrepo_id = \"bartowski/Qwen2.5-3B-Instruct-GGUF\"\nfilename = \"Qwen2.5-3B-Instruct-Q4_K_M.gguf\"\n\nmodel_path = hf_hub_download(repo_id=repo_id, filename=filename)\nprint(f\"Model downloaded: {model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:22:14.627675Z","iopub.execute_input":"2026-02-09T08:22:14.628449Z","iopub.status.idle":"2026-02-09T08:22:19.015082Z","shell.execute_reply.started":"2026-02-09T08:22:14.628420Z","shell.execute_reply":"2026-02-09T08:22:19.014533Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Qwen2.5-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c96fd82bf1c4c62a67bd280cd6fbca3"}},"metadata":{}},{"name":"stdout","text":"Model downloaded: /root/.cache/huggingface/hub/models--bartowski--Qwen2.5-3B-Instruct-GGUF/snapshots/f302c64a2269a69fb27b2f9473b362f5bb8e78d8/Qwen2.5-3B-Instruct-Q4_K_M.gguf\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================== \n# Step 6: Start Server (GPU 0)\n# ============================================================================== \nfrom llamatelemetry.server import ServerManager\nimport torch\n\nprint(f\"Found {torch.cuda.device_count()} GPUs:\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n\nserver = ServerManager(server_url=\"http://127.0.0.1:8090\")\nserver.start_server(\n    model_path=model_path,\n    gpu_layers=99,\n    tensor_split=\"1.0,0.0\",  # GPU 0 only\n    flash_attn=1,\n    port=8090,\n    host=\"127.0.0.1\",\n    ctx_size=4096,\n    batch_size=512,\n)\n\nprint(\"Server running on http://127.0.0.1:8090\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:22:24.796731Z","iopub.execute_input":"2026-02-09T08:22:24.797437Z","iopub.status.idle":"2026-02-09T08:22:30.714691Z","shell.execute_reply.started":"2026-02-09T08:22:24.797407Z","shell.execute_reply":"2026-02-09T08:22:30.713955Z"}},"outputs":[{"name":"stdout","text":"Found 2 GPUs:\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Qwen2.5-3B-Instruct-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready...... âœ“ Ready in 3.0s\nServer running on http://127.0.0.1:8090\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"### **Step 7: Instrumented Inference (Silent)**\n\nfrom llamatelemetry.api import LlamaCppClient\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\nimport time\n\nclass InstrumentedLLMClient:\n    \"\"\"LLM client with OpenTelemetry instrumentation\"\"\"\n\n    def __init__(self, base_url: str, tracer, meter):\n        self.client = LlamaCppClient(base_url)\n        self.tracer = tracer\n        self.request_counter = request_counter\n        self.latency_histogram = latency_histogram\n        self.token_histogram = token_histogram\n\n    def chat_completion(self, messages: list, **kwargs):\n        model = kwargs.get(\"model\", \"unknown\")\n        max_tokens = kwargs.get(\"max_tokens\", 100)\n        temperature = kwargs.get(\"temperature\", 0.7)\n\n        with self.tracer.start_as_current_span(\n            name=f\"llm.chat.{model}\",\n            kind=trace.SpanKind.CLIENT,\n        ) as span:\n            try:\n                span.set_attribute(\"llm.system\", \"llama.cpp\")\n                span.set_attribute(\"llm.model\", model)\n                span.set_attribute(\"llm.request.max_tokens\", max_tokens)\n                span.set_attribute(\"llm.request.temperature\", temperature)\n                span.set_attribute(\"llm.request.messages\", len(messages))\n\n                start_time = time.time()\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    **kwargs\n                )\n                latency_ms = (time.time() - start_time) * 1000\n\n                finish_reason = response.choices[0].finish_reason\n                content = response.choices[0].message.content\n\n                span.set_attribute(\"llm.response.finish_reason\", finish_reason)\n                span.set_attribute(\"llm.response.length\", len(content))\n\n                self.request_counter.add(\n                    1,\n                    attributes={\n                        \"model\": model,\n                        \"finish_reason\": finish_reason,\n                        \"status\": \"success\",\n                    }\n                )\n                self.latency_histogram.record(\n                    latency_ms,\n                    attributes={\"model\": model, \"status\": \"success\"}\n                )\n\n                if hasattr(response, 'usage'):\n                    input_tokens = getattr(response.usage, 'prompt_tokens', 0)\n                    output_tokens = getattr(response.usage, 'completion_tokens', 0)\n\n                    span.set_attribute(\"llm.usage.input_tokens\", input_tokens)\n                    span.set_attribute(\"llm.usage.output_tokens\", output_tokens)\n\n                    self.token_histogram.record(\n                        input_tokens,\n                        attributes={\"model\": model, \"token_type\": \"input\"}\n                    )\n                    self.token_histogram.record(\n                        output_tokens,\n                        attributes={\"model\": model, \"token_type\": \"output\"}\n                    )\n\n                span.set_status(Status(StatusCode.OK))\n                return response\n\n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                self.request_counter.add(\n                    1,\n                    attributes={\n                        \"model\": model,\n                        \"status\": \"error\",\n                        \"error_type\": type(e).__name__,\n                    }\n                )\n                raise\n\n# Initialize instrumented client\nllm = InstrumentedLLMClient(\"http://127.0.0.1:8090\", tracer, meter)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:22:30.716075Z","iopub.execute_input":"2026-02-09T08:22:30.717551Z","iopub.status.idle":"2026-02-09T08:22:30.747240Z","shell.execute_reply.started":"2026-02-09T08:22:30.717502Z","shell.execute_reply":"2026-02-09T08:22:30.746389Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#** Step 8: Generate Sample Requests**\n\n# Collect telemetry data for visualization\nfrom opentelemetry.sdk.trace.export.in_memory_span_exporter import InMemorySpanExporter\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry import trace\n\nmemory_exporter = InMemorySpanExporter()\ntracer_provider.add_span_processor(SimpleSpanProcessor(memory_exporter))\n\nimport random\nimport time\n\n# Wrap batches in a parent span so child spans have parents\nwith tracer.start_as_current_span(\"llm.batch.requests\"):\n    response = llm.chat_completion(\n        messages=[{\"role\": \"user\", \"content\": \"What is CUDA?\"}],\n        max_tokens=100,\n        temperature=0.7,\n    )\n    print(f\"Response: {response.choices[0].message.content}\")\n\n    prompts = [\n        \"Explain transformer architecture\",\n        \"What is quantization in LLMs?\",\n        \"How does FlashAttention work?\",\n        \"Describe the attention mechanism\",\n        \"What is GGUF format?\",\n    ]\n\n    responses = []\n    for i, prompt in enumerate(prompts):\n        print(f\"Request {i+1}/{len(prompts)}: {prompt[:50]}...\")\n        resp = llm.chat_completion(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=random.randint(50, 150),\n            temperature=random.uniform(0.5, 0.9),\n        )\n        responses.append(resp)\n        time.sleep(0.5)\n\n    print(f\"Completed {len(responses)} requests\")\n\nwith tracer.start_as_current_span(\"llm.batch.test\"):\n    for i in range(10):\n        llm.chat_completion(\n            messages=[{\"role\": \"user\", \"content\": f\"Test request {i}\"}],\n            max_tokens=50,\n        )\n\ntry:\n    tracer_provider.force_flush()\nexcept Exception:\n    pass\n\nfinished_spans = memory_exporter.get_finished_spans()\nprint(f\"Captured {len(finished_spans)} spans\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:22:43.092157Z","iopub.execute_input":"2026-02-09T08:22:43.092739Z","iopub.status.idle":"2026-02-09T08:23:01.102640Z","shell.execute_reply.started":"2026-02-09T08:22:43.092708Z","shell.execute_reply":"2026-02-09T08:23:01.101885Z"}},"outputs":[{"name":"stdout","text":"Response: CUDA stands for Compute Unified Device Architecture, and it is a parallel computing platform and application programming interface (API) model developed by NVIDIA. It allows developers to use a GPU (Graphics Processing Unit) not just for its graphics capabilities but also for general-purpose computing tasks.\n\nKey points about CUDA:\n\n1. Purpose: To enable the utilization of GPUs as powerful accelerators that can speed up compute-bound applications.\n\n2. Architecture: CUDA works with NVIDIA GPUs, which are designed specifically to support parallel processing and have\nRequest 1/5: Explain transformer architecture...\nRequest 2/5: What is quantization in LLMs?...\nRequest 3/5: How does FlashAttention work?...\nRequest 4/5: Describe the attention mechanism...\nRequest 5/5: What is GGUF format?...\nCompleted 5 requests\nCaptured 18 spans\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==============================================================================\n# Step 8b: Grafana OTLP Export Sanity Check\n# ==============================================================================\nimport time\n\ntry:\n    tracer_provider.force_flush()\nexcept Exception:\n    pass\n\ntry:\n    meter_provider.force_flush()\nexcept Exception:\n    pass\n\nprint('OTel force_flush called; waiting 10s for export...')\ntime.sleep(10)\nprint('If Grafana is configured, traces/metrics should now appear in your dashboards.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:23:05.644572Z","iopub.execute_input":"2026-02-09T08:23:05.644876Z","iopub.status.idle":"2026-02-09T08:23:16.139644Z","shell.execute_reply.started":"2026-02-09T08:23:05.644848Z","shell.execute_reply":"2026-02-09T08:23:16.138869Z"}},"outputs":[{"name":"stdout","text":"OTel force_flush called; waiting 10s for export...\nIf Grafana is configured, traces/metrics should now appear in your dashboards.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================== \n# Step 9: Metrics Collector (local, non-OTel) - reuse from notebook 15\n# ============================================================================== \nimport requests\nimport time\nimport re\nfrom typing import Dict, List\nfrom collections import defaultdict\nimport threading\nimport pandas as pd\n\nclass LlamaMetricsCollector:\n    def __init__(self, base_url: str = \"http://127.0.0.1:8090\"):\n        self.base_url = base_url\n        self.metrics_history = defaultdict(list)\n        self.slots_history = []\n        self.gpu_metrics_history = []\n        self.timestamps = []\n        self.running = False\n        self.lock = threading.Lock()\n\n    def parse_prometheus_metrics(self, text: str) -> Dict[str, float]:\n        metrics = {}\n        for line in text.split(\"\\n\"):\n            if line.startswith(\"#\") or not line.strip():\n                continue\n            match = re.match(r\"(\\w+)\\s+([\\d.]+)\", line)\n            if match:\n                metric_name, value = match.groups()\n                metrics[metric_name] = float(value)\n        return metrics\n\n    def fetch_server_metrics(self) -> Dict[str, float]:\n        try:\n            response = requests.get(f\"{self.base_url}/metrics\", timeout=2)\n            if response.status_code == 200:\n                return self.parse_prometheus_metrics(response.text)\n        except Exception:\n            pass\n        return {}\n\n    def fetch_slots_info(self) -> List[Dict]:\n        try:\n            response = requests.get(f\"{self.base_url}/slots\", timeout=2)\n            if response.status_code == 200:\n                return response.json()\n        except Exception:\n            pass\n        return []\n\n    def fetch_gpu_metrics(self) -> Dict[str, float]:\n        try:\n            import pynvml\n            try:\n                pynvml.nvmlInit()\n            except:\n                pass\n            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n            temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n            power_draw = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000\n            return {\n                \"gpu_utilization\": utilization.gpu,\n                \"memory_utilization\": utilization.memory,\n                \"memory_used_mb\": memory_info.used / 1024**2,\n                \"memory_total_mb\": memory_info.total / 1024**2,\n                \"temperature_c\": temperature,\n                \"power_draw_w\": power_draw,\n            }\n        except Exception:\n            return {}\n\n    def collect_once(self):\n        timestamp = time.time()\n        server_metrics = self.fetch_server_metrics()\n        slots_info = self.fetch_slots_info()\n        gpu_metrics = self.fetch_gpu_metrics()\n\n        with self.lock:\n            self.timestamps.append(timestamp)\n            for key, value in server_metrics.items():\n                self.metrics_history[key].append(value)\n            self.slots_history.append({\n                \"timestamp\": timestamp,\n                \"slots\": slots_info,\n                \"num_processing\": sum(1 for s in slots_info if s.get(\"is_processing\", False)),\n                \"num_idle\": sum(1 for s in slots_info if not s.get(\"is_processing\", False)),\n            })\n            self.gpu_metrics_history.append({\"timestamp\": timestamp, **gpu_metrics})\n\n    def start_background_collection(self, interval: float = 1.0):\n        self.running = True\n        def collect_loop():\n            while self.running:\n                self.collect_once()\n                time.sleep(interval)\n        thread = threading.Thread(target=collect_loop, daemon=True)\n        thread.start()\n        print(f\"Started metrics collection (interval={interval}s)\")\n\n    def stop_background_collection(self):\n        self.running = False\n        print(\"Stopped metrics collection\")\n\n    def get_gpu_dataframe(self) -> pd.DataFrame:\n        with self.lock:\n            if not self.gpu_metrics_history:\n                return pd.DataFrame()\n            return pd.DataFrame(self.gpu_metrics_history)\n\ncollector = LlamaMetricsCollector()\ncollector.collect_once()\nprint(\"Metrics collector initialized\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:23:54.285903Z","iopub.execute_input":"2026-02-09T08:23:54.286496Z","iopub.status.idle":"2026-02-09T08:23:54.310693Z","shell.execute_reply.started":"2026-02-09T08:23:54.286467Z","shell.execute_reply":"2026-02-09T08:23:54.310065Z"}},"outputs":[{"name":"stdout","text":"Metrics collector initialized\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================================== \n# Step 10: Generate Continuous Inference Load (non-JS plotting)\n# ============================================================================== \nimport threading\n\nclass LoadGenerator:\n    def __init__(self, client, interval=1.0):\n        self.client = client\n        self.interval = interval\n        self.running = False\n        self.thread = None\n\n    def _loop(self):\n        prompts = [\n            \"Explain CUDA in simple terms.\",\n            \"What is GGUF?\",\n            \"Summarize LLM quantization.\",\n            \"Describe FlashAttention.\",\n        ]\n        i = 0\n        while self.running:\n            prompt = prompts[i % len(prompts)]\n            try:\n                self.client.chat_completion(\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    max_tokens=120,\n                    temperature=0.7,\n                )\n            except Exception:\n                pass\n            i += 1\n            time.sleep(self.interval)\n\n    def start(self):\n        self.running = True\n        self.thread = threading.Thread(target=self._loop, daemon=True)\n        self.thread.start()\n        print(\"Load generator started\")\n\n    def stop(self):\n        self.running = False\n        print(\"Load generator stopped\")\n\nload_gen = LoadGenerator(llm, interval=1.5)\ncollector.start_background_collection(interval=1.0)\nload_gen.start()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:31:46.901576Z","iopub.execute_input":"2026-02-09T08:31:46.902099Z","iopub.status.idle":"2026-02-09T08:31:46.915853Z","shell.execute_reply.started":"2026-02-09T08:31:46.902068Z","shell.execute_reply":"2026-02-09T08:31:46.913200Z"}},"outputs":[{"name":"stdout","text":"Started metrics collection (interval=1.0s)\nLoad generator started\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ============================================================================== \n# Step 12: Stop Everything\n# ============================================================================== \nload_gen.stop()\ncollector.stop_background_collection()\nserver.stop_server()\nprint(\"Cleanup complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:31:47.712692Z","iopub.execute_input":"2026-02-09T08:31:47.713457Z","iopub.status.idle":"2026-02-09T08:31:58.033971Z","shell.execute_reply.started":"2026-02-09T08:31:47.713414Z","shell.execute_reply":"2026-02-09T08:31:58.033288Z"}},"outputs":[{"name":"stdout","text":"Load generator stopped\nStopped metrics collection\nCleanup complete\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}